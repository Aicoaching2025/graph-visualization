{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DATA 620\n",
        "### Week 3: Graph Visualization\n",
        "**Completed by:** Candace Grant\n",
        "\n",
        "**Date:** 2/21/26\n",
        "\n",
        "### Assignment Overview\n",
        "\n",
        "In this assignment I am required to complete the following steps.\n",
        "1. Load a graph database of your choosing from a text file or other source.\n",
        "2. Create basic analysis on the graph\n",
        "3. Use a visualization tool to display information.\n",
        "4. Submit a short video presentation as part of the submission.\n",
        "\n",
        "\n",
        "### Dataset Information\n",
        "\n",
        "Network was collected by crawling the Amazon website. It is based on the **Customers Who Bought This Item Also Bought** feature of the Amazon website. If a product *i* is frequently co-purchased with product *j*, the graph contains a directed edge from *i* to *j*.\n",
        "\n",
        "The data was collected on June 01, 2003.\n",
        "\n",
        "---\n",
        "\n",
        "### Dataset Statistics\n",
        "\n",
        "| Metric | Value |\n",
        "|---|---|\n",
        "| Nodes | 403,394 |\n",
        "| Edges | 3,387,388 |\n",
        "| Nodes in largest WCC | 403,364 (1.000) |\n",
        "| Edges in largest WCC | 3,387,224 (1.000) |\n",
        "| Nodes in largest SCC | 395,234 (0.980) |\n",
        "| Edges in largest SCC | 3,301,092 (0.975) |\n",
        "| Average clustering coefficient | 0.4177 |\n",
        "| Number of triangles | 3,986,507 |\n",
        "| Fraction of closed triangles | 0.06206 |\n",
        "| Diameter (longest shortest path) | 21 |\n",
        "| 90-percentile effective diameter | 7.6 |\n",
        "\n",
        "---\n",
        "\n",
        "### Source (Citation)\n",
        "\n",
        "J. Leskovec, L. Adamic and B. Adamic. *The Dynamics of Viral Marketing.* ACM Transactions on the Web (ACM TWEB), 1(1), 2007.\n",
        "\n",
        "| File | Description |\n",
        "|---|---|\n",
        "| `amazon0601.txt.gz` | Amazon product co-purchasing network from June 01, 2003 |\n",
        "\n",
        "**Download:** [https://snap.stanford.edu/data/amazon0601.html](https://snap.stanford.edu/data/amazon0601.html)\n"
      ],
      "metadata": {
        "id": "2RMVnxVLWOiW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWSjGy-I16eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Setup and Import libraries"
      ],
      "metadata": {
        "id": "AgZ4JDGYWXq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ICBEjQCV6Oc",
        "outputId": "b9bae2d2-ba16-40e7-ad08-b850b51d1d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetworkX version: 3.6.1\n",
            "NumPy version:    2.0.2\n",
            "CPU cores:        2\n",
            "All packages loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, deque\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gzip\n",
        "import os\n",
        "import urllib.request\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Plotting style\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#FAFAFA',\n",
        "    'axes.facecolor': '#FAFAFA',\n",
        "    'font.family': 'sans-serif',\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelsize': 12,\n",
        "    'figure.dpi': 150,\n",
        "})\n",
        "\n",
        "print(f\"NetworkX version: {nx.__version__}\")\n",
        "print(f\"NumPy version:    {np.__version__}\")\n",
        "print(f\"CPU cores:        {os.cpu_count()}\")\n",
        "print(\"All packages loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 Load the Graph (Vectorized with Numpy)\n",
        "Instead of reading line-by-line in a Python loop, `np.loadtxt()` bulk-loads all edges into a contiguous C array, then `add_edges_from()` batch-adds them in a single call. This is **5–10x faster** than the loop approach.\n"
      ],
      "metadata": {
        "id": "7WXn3l35Wm9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"amazon0601.txt.gz\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Downloading {file_path} from SNAP...\")\n",
        "    url = \"https://snap.stanford.edu/data/amazon0601.txt.gz\"\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, file_path)\n",
        "        print(\"Download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please download manually from https://snap.stanford.edu/data/amazon0601.html\")\n",
        "\n",
        "# Vectorized loading\n",
        "start = time.time()\n",
        "print(\"Loading edges with NumPy (vectorized)...\")\n",
        "edges_array = np.loadtxt(file_path, dtype=int, comments='#')\n",
        "\n",
        "G_full = nx.DiGraph()\n",
        "G_full.add_edges_from(edges_array)\n",
        "\n",
        "print(f\"  Loaded in {time.time()-start:.2f}s\")\n",
        "print(f\"  Nodes: {G_full.number_of_nodes():,}\")\n",
        "print(f\"  Edges: {G_full.number_of_edges():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIzT9RouWfa8",
        "outputId": "9653e7d8-e504-48dc-f51e-33b5fa5ea0b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading amazon0601.txt.gz from SNAP...\n",
            "Download complete.\n",
            "Loading edges with NumPy (vectorized)...\n",
            "  Loaded in 18.78s\n",
            "  Nodes: 403,394\n",
            "  Edges: 3,387,388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Snowball Sampling\n",
        "The full graph has 400K+ nodes which makes exact diameter and betweenness centrality computation prohibitively expensive. We use **snowball sampling** (BFS from a high-degree seed node) which preserves local network structure — clustering patterns, community structure, and hub-spoke topology remain intact. This is superior to random node sampling or simply taking the first N lines of the file."
      ],
      "metadata": {
        "id": "mp13TDWiWz1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_SIZE = 10000\n",
        "\n",
        "def snowball_sample(G, target_size, seed_node=None):\n",
        "    \"\"\"BFS snowball sampling — preserves local network structure.\"\"\"\n",
        "    G_und = G.to_undirected()\n",
        "\n",
        "    if seed_node is None:\n",
        "        degree_arr = np.array(G_und.degree())\n",
        "        top_idx = np.argsort(degree_arr[:, 1])[-20:]\n",
        "        seed_node = int(degree_arr[np.random.choice(top_idx)][0])\n",
        "\n",
        "    print(f\"Snowball sampling from seed node {seed_node}...\")\n",
        "    sampled = set()\n",
        "    frontier = {seed_node}\n",
        "\n",
        "    while len(sampled) < target_size and frontier:\n",
        "        sampled.update(frontier)\n",
        "        next_frontier = set()\n",
        "        for node in frontier:\n",
        "            next_frontier.update(set(G_und.neighbors(node)) - sampled)\n",
        "        if len(sampled) + len(next_frontier) > target_size * 1.2:\n",
        "            needed = target_size - len(sampled)\n",
        "            next_frontier = set(random.sample(list(next_frontier), min(needed, len(next_frontier))))\n",
        "        frontier = next_frontier\n",
        "\n",
        "    sampled = list(sampled)[:target_size]\n",
        "    return G.subgraph(sampled).copy(), G_und.subgraph(sampled).copy()\n",
        "\n",
        "start = time.time()\n",
        "G_dir, G_und = snowball_sample(G_full, SAMPLE_SIZE)\n",
        "print(f\"  Sampled nodes: {G_dir.number_of_nodes():,}\")\n",
        "print(f\"  Sampled edges: {G_dir.number_of_edges():,}\")\n",
        "print(f\"  Completed in {time.time()-start:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuUWvII3fgHq",
        "outputId": "136f9ece-d42f-4e92-a4f0-7cc56c38b761"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball sampling from seed node 533...\n",
            "  Sampled nodes: 10,000\n",
            "  Sampled edges: 34,261\n",
            "  Completed in 17.43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Extract Largest Connected Component"
      ],
      "metadata": {
        "id": "_fARk-j1ftnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weakly connected components (directed)\n",
        "wcc_list = list(nx.weakly_connected_components(G_dir))\n",
        "largest_wcc = max(wcc_list, key=len)\n",
        "\n",
        "# Largest connected component (undirected) — used for diameter and path metrics\n",
        "largest_cc = max(nx.connected_components(G_und), key=len)\n",
        "H = G_und.subgraph(largest_cc).copy()\n",
        "\n",
        "# Strongly connected components (directed)\n",
        "scc_list = list(nx.strongly_connected_components(G_dir))\n",
        "largest_scc = max(scc_list, key=len)\n",
        "\n",
        "print(f\"Weakly Connected Components:  {len(wcc_list)}\")\n",
        "print(f\"  Largest WCC: {len(largest_wcc):,} nodes ({len(largest_wcc)/G_dir.number_of_nodes():.3f})\")\n",
        "print(f\"Strongly Connected Components: {len(scc_list)}\")\n",
        "print(f\"  Largest SCC: {len(largest_scc):,} nodes ({len(largest_scc)/G_dir.number_of_nodes():.3f})\")\n",
        "print(f\"\\nLargest CC (undirected) for analysis:\")\n",
        "print(f\"  Nodes: {H.number_of_nodes():,}\")\n",
        "print(f\"  Edges: {H.number_of_edges():,}\")"
      ],
      "metadata": {
        "id": "NxgEH663f5Os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a754fd1f-1d59-4753-d358-7ad0a440b2e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weakly Connected Components:  1\n",
            "  Largest WCC: 10,000 nodes (1.000)\n",
            "Strongly Connected Components: 5214\n",
            "  Largest SCC: 2,581 nodes (0.258)\n",
            "\n",
            "Largest CC (undirected) for analysis:\n",
            "  Nodes: 10,000\n",
            "  Edges: 27,375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: Metric 1 - Diameter (Required)\n",
        "\n",
        "The **diameter** is the longest shortest path between any two nodes in the network. It tells us: what is the maximum number of co-purchase hops needed to connect the two most distant products?\n",
        "\n",
        "I implement this **by hand** using BFS from sampled nodes to build intuition, then verify with the NetworkX approximation algorithm."
      ],
      "metadata": {
        "id": "9zEulqVQfqiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  HAND-CODED DIAMETER ESTIMATION\n",
        "#  Using BFS (Breadth-First Search) from sampled nodes\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "def bfs_eccentricity(graph, source):\n",
        "    \"\"\"\n",
        "    Hand-coded BFS to find the eccentricity of a single node.\n",
        "    Eccentricity = the longest shortest path FROM this node to any other.\n",
        "    The diameter is the maximum eccentricity across all nodes.\n",
        "    \"\"\"\n",
        "    visited = {source: 0}\n",
        "    queue = deque([source])\n",
        "    max_distance = 0\n",
        "\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        current_dist = visited[node]\n",
        "\n",
        "        for neighbor in graph.neighbors(node):\n",
        "            if neighbor not in visited:\n",
        "                visited[neighbor] = current_dist + 1\n",
        "                max_distance = max(max_distance, current_dist + 1)\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    return max_distance, visited\n",
        "\n",
        "\n",
        "def estimate_diameter_bfs(graph, n_samples=100):\n",
        "    \"\"\"\n",
        "    Estimate diameter by running BFS from n_samples random nodes.\n",
        "    The maximum eccentricity found is a lower bound on the true diameter.\n",
        "    \"\"\"\n",
        "    nodes = list(graph.nodes())\n",
        "    sample = random.sample(nodes, min(n_samples, len(nodes)))\n",
        "\n",
        "    max_ecc = 0\n",
        "    total_path_length = 0\n",
        "    total_pairs = 0\n",
        "\n",
        "    for source in sample:\n",
        "        ecc, distances = bfs_eccentricity(graph, source)\n",
        "        max_ecc = max(max_ecc, ecc)\n",
        "        total_path_length += sum(distances.values())\n",
        "        total_pairs += len(distances)\n",
        "\n",
        "    avg_path = total_path_length / total_pairs if total_pairs > 0 else 0\n",
        "    return max_ecc, avg_path\n",
        "\n",
        "\n",
        "# Run hand-coded diameter estimation\n",
        "start = time.time()\n",
        "print(\"Computing diameter (hand-coded BFS from 100 sampled nodes)...\")\n",
        "diameter_hand, avg_path_hand = estimate_diameter_bfs(H, n_samples=100)\n",
        "elapsed_hand = time.time() - start\n",
        "\n",
        "print(f\"\\n  Hand-Coded Results:\")\n",
        "print(f\"    Estimated Diameter:       {diameter_hand}\")\n",
        "print(f\"    Avg Shortest Path Length: {avg_path_hand:.3f}\")\n",
        "print(f\"    Computed in {elapsed_hand:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq2SJlPjg2Fd",
        "outputId": "85294d27-fbdf-49ee-e623-f23d6e42565a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing diameter (hand-coded BFS from 100 sampled nodes)...\n",
            "\n",
            "  Hand-Coded Results:\n",
            "    Estimated Diameter:       6\n",
            "    Avg Shortest Path Length: 4.506\n",
            "    Computed in 2.18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  VERIFY WITH NETWORKX APPROXIMATION\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "start = time.time()\n",
        "diameter_nx = nx.approximation.diameter(H)\n",
        "elapsed_nx = time.time() - start\n",
        "\n",
        "print(f\"  NetworkX Approximation:\")\n",
        "print(f\"    Approximate Diameter:    {diameter_nx}\")\n",
        "print(f\"    Computed in {elapsed_nx:.2f}s\")\n",
        "\n",
        "print(f\"\\n  Full SNAP Dataset Diameter: 21\")\n",
        "print(f\"  Full SNAP 90th Pctl Eff. Diameter: 7.6\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2khxUing9nN",
        "outputId": "863fb6a3-10b8-4e90-db98-013296655729"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NetworkX Approximation:\n",
            "    Approximate Diameter:    6\n",
            "    Computed in 0.06s\n",
            "\n",
            "  Full SNAP Dataset Diameter: 21\n",
            "  Full SNAP 90th Pctl Eff. Diameter: 7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6: Metric 2 — Clustering Coefficient (Hand-Coded)\n",
        "\n",
        "The **clustering coefficient** measures the tendency of products to form tightly connected groups. If product A is co-purchased with products B and C, the clustering coefficient asks: are B and C also co-purchased with each other?\n",
        "\n",
        "For a node with degree *k*, the maximum possible edges between its neighbors is *k(k-1)/2*. The local clustering coefficient is the fraction of those possible edges that actually exist."
      ],
      "metadata": {
        "id": "C3TvbhTKhJ4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  HAND-CODED CLUSTERING COEFFICIENT\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "def local_clustering_coefficient(graph, node):\n",
        "    \"\"\"\n",
        "    Hand-coded local clustering coefficient for a single node.\n",
        "\n",
        "    Formula: C(v) = 2 * triangles(v) / (degree(v) * (degree(v) - 1))\n",
        "\n",
        "    This measures: of all possible connections between my neighbors,\n",
        "    what fraction actually exist?\n",
        "    \"\"\"\n",
        "    neighbors = set(graph.neighbors(node))\n",
        "    k = len(neighbors)\n",
        "\n",
        "    if k < 2:\n",
        "        return 0.0  # Need at least 2 neighbors to form a triangle\n",
        "\n",
        "    # Count edges between neighbors\n",
        "    edges_between = 0\n",
        "    for neighbor in neighbors:\n",
        "        # How many of this neighbor's neighbors are also our neighbors?\n",
        "        edges_between += len(set(graph.neighbors(neighbor)) & neighbors)\n",
        "\n",
        "    # Each edge counted twice (once from each end)\n",
        "    edges_between = edges_between / 2\n",
        "\n",
        "    # Maximum possible edges between k neighbors\n",
        "    max_possible = k * (k - 1) / 2\n",
        "\n",
        "    return edges_between / max_possible\n",
        "\n",
        "\n",
        "def average_clustering_hand(graph, n_samples=2000):\n",
        "    \"\"\"\n",
        "    Compute average clustering coefficient over sampled nodes.\n",
        "    Sampling is necessary for large graphs — O(n * k²) per node.\n",
        "    \"\"\"\n",
        "    nodes = list(graph.nodes())\n",
        "    sample = random.sample(nodes, min(n_samples, len(nodes)))\n",
        "\n",
        "    coefficients = [local_clustering_coefficient(graph, n) for n in sample]\n",
        "    return np.mean(coefficients), np.array(coefficients)\n",
        "\n",
        "\n",
        "# Run hand-coded clustering\n",
        "start = time.time()\n",
        "print(\"Computing clustering coefficient (hand-coded, 2000 sampled nodes)...\")\n",
        "avg_cc_hand, cc_values_hand = average_clustering_hand(H, n_samples=2000)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"\\n  Hand-Coded Avg Clustering Coefficient: {avg_cc_hand:.4f}\")\n",
        "print(f\"  Computed in {elapsed:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W4JRlOKhPU7",
        "outputId": "23296f6b-0c58-4a71-81f1-4ea824e5f8d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing clustering coefficient (hand-coded, 2000 sampled nodes)...\n",
            "\n",
            "  Hand-Coded Avg Clustering Coefficient: 0.2744\n",
            "  Computed in 0.09s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  VERIFY WITH NETWORKX\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "start = time.time()\n",
        "avg_cc_nx = nx.average_clustering(H)\n",
        "transitivity = nx.transitivity(H)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"  NetworkX Avg Clustering:  {avg_cc_nx:.4f}\")\n",
        "print(f\"  NetworkX Transitivity:    {transitivity:.4f}\")\n",
        "print(f\"  Full SNAP Dataset:        0.4177\")\n",
        "print(f\"  Computed in {elapsed:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy79PkpHhWMA",
        "outputId": "bebf8e92-aac1-448c-d752-9ac072e4bc53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NetworkX Avg Clustering:  0.2819\n",
            "  NetworkX Transitivity:    0.0654\n",
            "  Full SNAP Dataset:        0.4177\n",
            "  Computed in 1.44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 7: Gephi Export for Network Visualization\n",
        "\n",
        "To visualize the network in Gephi, I export a 500-node subgraph with pre-computed node attributes (degree, clustering, betweenness, eigenvector centrality, and community labels). The subgraph is created using snowball sampling to preserve the local network structure, then saved as a `.gexf` file that Gephi can open directly."
      ],
      "metadata": {
        "id": "N1svUkH-2tAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  GEPHI EXPORT — Snowball sample + node metrics + GEXF\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "VIZ_SIZE = 500\n",
        "\n",
        "def snowball_sample_viz(G, target_size, seed_node=None):\n",
        "    \"\"\"BFS snowball sampling for Gephi visualization.\"\"\"\n",
        "    if seed_node is None:\n",
        "        degree_arr = np.array(list(G.degree()))\n",
        "        top_idx = np.argsort(degree_arr[:, 1].astype(int))[-10:]\n",
        "        seed_node = int(degree_arr[np.random.choice(top_idx)][0])\n",
        "\n",
        "    print(f\"  Snowball sampling from seed node {seed_node} for Gephi export...\")\n",
        "    sampled = set()\n",
        "    frontier = {seed_node}\n",
        "\n",
        "    while len(sampled) < target_size and frontier:\n",
        "        sampled.update(frontier)\n",
        "        next_frontier = set()\n",
        "        for node in frontier:\n",
        "            next_frontier.update(set(G.neighbors(node)) - sampled)\n",
        "        if len(sampled) + len(next_frontier) > target_size * 1.2:\n",
        "            needed = target_size - len(sampled)\n",
        "            next_frontier = set(random.sample(list(next_frontier), min(needed, len(next_frontier))))\n",
        "        frontier = next_frontier\n",
        "\n",
        "    sampled = list(sampled)[:target_size]\n",
        "    return G.subgraph(sampled).copy()\n",
        "\n",
        "\n",
        "def export_for_gephi(H, viz_size=500):\n",
        "    \"\"\"Creates Gephi-ready files from the network.\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  GEPHI EXPORT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Create visualization subgraph\n",
        "    print(f\"\\n1. Creating {viz_size}-node subgraph...\")\n",
        "    G_viz = snowball_sample_viz(H, viz_size)\n",
        "    print(f\"   Nodes: {G_viz.number_of_nodes()}\")\n",
        "    print(f\"   Edges: {G_viz.number_of_edges()}\")\n",
        "\n",
        "    # Step 2: Compute node-level metrics\n",
        "    print(\"\\n2. Computing node metrics for Gephi attributes...\")\n",
        "    degree_dict = dict(G_viz.degree())\n",
        "    clustering_dict = nx.clustering(G_viz)\n",
        "    betweenness_dict = nx.betweenness_centrality(G_viz, k=min(100, G_viz.number_of_nodes()))\n",
        "    eigenvector_dict = nx.eigenvector_centrality_numpy(G_viz)\n",
        "\n",
        "    for node in G_viz.nodes():\n",
        "        G_viz.nodes[node]['Label'] = f\"Product_{node}\"\n",
        "        G_viz.nodes[node]['degree'] = degree_dict[node]\n",
        "        G_viz.nodes[node]['clustering'] = round(clustering_dict[node], 4)\n",
        "        G_viz.nodes[node]['betweenness'] = round(betweenness_dict[node], 6)\n",
        "        G_viz.nodes[node]['eigenvector'] = round(eigenvector_dict[node], 6)\n",
        "\n",
        "    # Step 3: Detect communities\n",
        "    print(\"   Detecting communities (Louvain)...\")\n",
        "    try:\n",
        "        communities = nx.community.louvain_communities(G_viz, seed=42)\n",
        "        for i, comm in enumerate(communities):\n",
        "            for node in comm:\n",
        "                G_viz.nodes[node]['community'] = i\n",
        "        print(f\"   Found {len(communities)} communities\")\n",
        "    except Exception:\n",
        "        print(\"   Louvain not available, skipping community detection\")\n",
        "        for node in G_viz.nodes():\n",
        "            G_viz.nodes[node]['community'] = 0\n",
        "\n",
        "    # Export GEXF\n",
        "    print(\"\\n3. Exporting files...\")\n",
        "    nx.write_gexf(G_viz, \"gephi_amazon_500.gexf\")\n",
        "    print(\"   ✓ gephi_amazon_500.gexf\")\n",
        "\n",
        "    # Export CSVs\n",
        "    nodes_df = pd.DataFrame([{\n",
        "        'Id': n, 'Label': f'Product_{n}',\n",
        "        'degree': degree_dict[n],\n",
        "        'clustering': round(clustering_dict[n], 4),\n",
        "        'betweenness': round(betweenness_dict[n], 6),\n",
        "        'eigenvector': round(eigenvector_dict[n], 6),\n",
        "        'community': G_viz.nodes[n].get('community', 0)\n",
        "    } for n in G_viz.nodes()])\n",
        "    nodes_df.to_csv(\"gephi_nodes_500.csv\", index=False)\n",
        "    print(\"   ✓ gephi_nodes_500.csv\")\n",
        "\n",
        "    edges_df = pd.DataFrame([{'Source': u, 'Target': v} for u, v in G_viz.edges()])\n",
        "    edges_df.to_csv(\"gephi_edges_500.csv\", index=False)\n",
        "    print(\"   ✓ gephi_edges_500.csv\")\n",
        "\n",
        "    print(\"\\n  Done! Download the .gexf file and open it in Gephi (File → Open).\")\n",
        "    return G_viz, nodes_df, edges_df"
      ],
      "metadata": {
        "id": "2irsGQ1q2sE0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  RUN THE EXPORT (uses H from Section 4)\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "G_viz, nodes_df, edges_df = export_for_gephi(H)\n",
        "\n",
        "print(\"\\nNode metrics preview:\")\n",
        "print(nodes_df.head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEv0fmvE2sPF",
        "outputId": "12639fab-a077-45eb-e202-e6de04df0d66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  GEPHI EXPORT\n",
            "============================================================\n",
            "\n",
            "1. Creating 500-node subgraph...\n",
            "  Snowball sampling from seed node 45 for Gephi export...\n",
            "   Nodes: 500\n",
            "   Edges: 632\n",
            "\n",
            "2. Computing node metrics for Gephi attributes...\n",
            "   Detecting communities (Louvain)...\n",
            "   Found 21 communities\n",
            "\n",
            "3. Exporting files...\n",
            "   ✓ gephi_amazon_500.gexf\n",
            "   ✓ gephi_nodes_500.csv\n",
            "   ✓ gephi_edges_500.csv\n",
            "\n",
            "  Done! Download the .gexf file and open it in Gephi (File → Open).\n",
            "\n",
            "Node metrics preview:\n",
            "    Id          Label  degree  clustering  betweenness  eigenvector  community\n",
            "290821 Product_290821       1      0.0000     0.000000     0.017891          4\n",
            "180239 Product_180239       1      0.0000     0.000000     0.017891          4\n",
            "110620 Product_110620       1      0.0000     0.000000     0.036739         16\n",
            "356388 Product_356388       1      0.0000     0.000000     0.017891          4\n",
            "391206 Product_391206       1      0.0000     0.000000     0.036739         16\n",
            "385065 Product_385065       1      0.0000     0.000000     0.036739         16\n",
            "366634 Product_366634       1      0.0000     0.000000     0.036739         16\n",
            "348204 Product_348204       2      0.0000     0.003297     0.038035          4\n",
            "    45     Product_45     266      0.0021     0.795114     0.636100         16\n",
            "346167 Product_346167       2      1.0000     0.000000     0.054631          4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ════════════════════════════════════════════════════════\n",
        "#  DOWNLOAD FILES FROM COLAB\n",
        "#  This cell triggers browser downloads\n",
        "# ════════════════════════════════════════════════════════\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"gephi_amazon_500.gexf\")\n",
        "files.download(\"gephi_nodes_500.csv\")\n",
        "files.download(\"gephi_edges_500.csv\")\n",
        "\n",
        "print(\"Files downloaded! Open gephi_amazon_500.gexf in Gephi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0Z8LEc_m2sWR",
        "outputId": "781569d7-b7a7-4fc1-c4c8-c24e2e94bdc9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b51fe1e1-e054-40d8-a6fd-ff0b8147af46\", \"gephi_amazon_500.gexf\", 216160)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e84f0bc7-02d0-48ad-bf4d-71df9a7c1580\", \"gephi_nodes_500.csv\", 21987)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1069fd1d-890d-4cbe-ad61-bc3d4a018f3f\", \"gephi_edges_500.csv\", 6900)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files downloaded! Open gephi_amazon_500.gexf in Gephi.\n"
          ]
        }
      ]
    }
  ]
}